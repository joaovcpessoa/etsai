{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e23462f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55082008",
   "metadata": {},
   "source": [
    "### Modelo\n",
    "\n",
    "Pelo que entendi, U-Net é uma rede neural convolucional totalmente convolucional (FCN) usada em segmentação semântica de imagens afim de prever uma classe para cada pixel da imagem.\n",
    "\n",
    "A arquitetura segue o formato de “U”, composta por duas partes principais:\n",
    "\n",
    "- Caminho de contração (encoder):\n",
    "- Camadas `DoubleConv` e `Down` reduzem progressivamente a resolução espacial (com MaxPool2d) enquanto aumentam o número de canais.\n",
    "- Extrai características de alto nível.\n",
    "\n",
    "Caminho de expansão (decoder):\n",
    "- Camadas `Up` aumentam a resolução com `Upsample` (ou `ConvTranspose2d`) e combinam (concatenação) os recursos do encoder via skip connections.\n",
    "- Permite que a rede recupere detalhes espaciais perdidos na compressão.\n",
    "\n",
    "Camada de saída (`OutConv`):\n",
    "- Usa `Conv2d(kernel_size=1)` para mapear o resultado final para n_classes, gerando o mapa de segmentação.\n",
    "\n",
    "O modelo recebe uma imagem, comprime suas informações em níveis profundos de abstração e reconstrói uma saída pixel a pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae00d0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels , in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        \n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = nn.functional.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
    "\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(512, 1024 // factor)\n",
    "        self.up1 = Up(1024, 512 // factor, bilinear)\n",
    "        self.up2 = Up(512, 256 // factor, bilinear)\n",
    "        self.up3 = Up(256, 128 // factor, bilinear)\n",
    "        self.up4 = Up(128, 64, bilinear)\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66dcd1d",
   "metadata": {},
   "source": [
    "### Geração de Mapas de Saliência\n",
    "\n",
    "O mapa de saliência é a própria probabilidade máxima, que indica a \"confiança\" do modelo em sua segmentação.<br>\n",
    "Áreas com alta probabilidade máxima são consideradas mais \"saliêntes\" para a decisão de segmentação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcfd3e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaliencyMapGenerator:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def generate(self, input_tensor):\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(input_tensor)\n",
    "\n",
    "        probabilities = torch.softmax(logits, dim=1)   # Converte logits para probabilidades (softmax)   \n",
    "        max_probs, _ = torch.max(probabilities, dim=1) # Encontra a probabilidade máxima para cada pixel\n",
    "        saliency_map = max_probs                       # Normaliza o mapa de saliência (0-1)\n",
    "\n",
    "        return saliency_map.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674d1c72",
   "metadata": {},
   "source": [
    "### Identificação de Áreas Relevantes e Re-treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f35ac3",
   "metadata": {},
   "source": [
    "A função `identify_relevant_areas` identifica os pixels mais relevantes com base no mapa de saliência e retorna uma máscara binária.\n",
    "\n",
    "A função `apply_spatial_reduction` simula a redução espacial (como no NeuLens) aplicando uma máscara. \n",
    "\n",
    "A máscara é (B, H, W). Expande para (B, 1, H, W) para multiplicação e no final simplesmente retorna a máscara para ser usada como peso na função de perda\n",
    "\n",
    "Em um cenário real, isso envolveria processar apenas os patches relevantes com uma sub-rede de alta resolução e o restante com uma sub-rede de baixa resolução/compressão. Aqui, vamos simular o \"reuso de dados\" ponderando a perda durante o re-treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96b53810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_relevant_areas(saliency_map, threshold=0.8):\n",
    "    relevant_mask = (saliency_map > threshold).astype(np.uint8)\n",
    "    return relevant_mask\n",
    "\n",
    "def apply_spatial_reduction(image_tensor, relevant_mask):\n",
    "    mask_tensor = torch.from_numpy(relevant_mask).unsqueeze(1).float().to(image_tensor.device)\n",
    "    return mask_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecc9017",
   "metadata": {},
   "source": [
    "### Função de Perda Ponderada\n",
    "\n",
    "Função de perda que aplica os pesos espaciais calculado a partir da máscara de saliência para dar mais importância às áreas relevantes.\n",
    "\n",
    "- prediction: (B, C, H, W)\n",
    "- target: (B, H, W)\n",
    "- weight_mask: (B, 1, H, W) - Máscara de pesos (saliência)\n",
    "\n",
    "Calcula a perda média apenas sobre os pixels relevantes (onde weight_mask > 0)\n",
    "\n",
    "Para evitar divisão por zero, adicionamos um pequeno epsilon\n",
    "\n",
    "Retorna a média normal se não houver pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd9f48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.base_loss = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "    def forward(self, prediction, target, weight_mask):\n",
    "        loss_per_pixel = self.base_loss(prediction, target)     # (B, H, W)\n",
    "        weighted_loss = loss_per_pixel * weight_mask.squeeze(1) # (B, H, W)\n",
    "        total_weight = weight_mask.sum()\n",
    "        if total_weight == 0:\n",
    "            return weighted_loss.mean()\n",
    "\n",
    "        return weighted_loss.sum() / total_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed582455",
   "metadata": {},
   "source": [
    "### Simulação de Dados e Treinamento\n",
    "\n",
    "Aqui temos a parte da montagem do Dataset de simulação para segmentação que retorna um tensor de imagem e um tensor de máscara de segmentação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51e180d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummySegmentationDataset(Dataset):\n",
    "    def __init__(self, num_samples=100, img_size=128, n_classes=2):\n",
    "        self.num_samples = num_samples\n",
    "        self.img_size = img_size\n",
    "        self.n_classes = n_classes\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Imagem simulada (3 canais)\n",
    "        image = np.random.rand(self.img_size, self.img_size, 3).astype(np.float32)\n",
    "        image = Image.fromarray((image * 255).astype(np.uint8))\n",
    "        image_tensor = self.transform(image)\n",
    "\n",
    "        # Máscara de segmentação simulada (n_classes)\n",
    "        # O target para CrossEntropyLoss deve ser (H, W) com valores de classe (0 a n_classes-1)\n",
    "        mask = np.random.randint(0, self.n_classes, (self.img_size, self.img_size), dtype=np.int64)\n",
    "        mask_tensor = torch.from_numpy(mask)\n",
    "\n",
    "        return image_tensor, mask_tensor\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device, iteration=0, saliency_generator=None, threshold=0.8):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, masks in dataloader:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Passo 1: Treinamento com segmentação semântica (normal ou ponderado)\n",
    "        outputs = model(images)\n",
    "\n",
    "        if iteration == 0:\n",
    "            # Iteração 0: Treinamento tradicional (Passo 1 do seu pipeline)\n",
    "            loss = nn.CrossEntropyLoss()(outputs, masks)\n",
    "            weight_mask = None\n",
    "        else:\n",
    "            # Iterações > 0: Treinamento ponderado (Passo 4 do seu pipeline)\n",
    "            # 2. Forma o mapa de saliência\n",
    "            saliency_map_np = saliency_generator.generate(images)\n",
    "\n",
    "            # 3. Identifica as áreas mais relevantes\n",
    "            relevant_mask_np = identify_relevant_areas(saliency_map_np, threshold)\n",
    "\n",
    "            # 4. Aplica a redução espacial (cria a máscara de pesos)\n",
    "            weight_mask = apply_spatial_reduction(images, relevant_mask_np)\n",
    "            weight_mask = weight_mask.to(device)\n",
    "\n",
    "            # Usa a função de perda ponderada\n",
    "            loss = criterion(outputs, masks, weight_mask)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "    return running_loss / len(dataloader.dataset)\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_pixels = 0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in dataloader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            total_pixels += masks.numel()\n",
    "            total_correct += (predicted == masks).sum().item()\n",
    "\n",
    "    accuracy = total_correct / total_pixels\n",
    "    return accuracy\n",
    "\n",
    "def iterative_training_pipeline(model, train_loader, val_loader, device, num_iterations=5, epochs_per_iteration=5, saliency_threshold=0.8):\n",
    "    \"\"\"\n",
    "    Implementa o pipeline de treinamento iterativo.\n",
    "    \"\"\"\n",
    "    print(f\"Iniciando pipeline de treinamento iterativo em {device}...\")\n",
    "\n",
    "    # Otimizador e Critério de Perda\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    weighted_criterion = WeightedCrossEntropyLoss()\n",
    "    saliency_generator = SaliencyMapGenerator(model)\n",
    "\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    for iteration in range(num_iterations):\n",
    "        print(f\"\\n--- Iteração {iteration} ---\")\n",
    "\n",
    "        if iteration == 0:\n",
    "            print(\"Fase 1: Treinamento inicial com segmentação semântica (tradicional).\")\n",
    "        else:\n",
    "            print(\"Fase 2-5: Re-treinamento com pesos de saliência.\")\n",
    "\n",
    "        for epoch in range(epochs_per_iteration):\n",
    "            loss = train_one_epoch(\n",
    "                model,\n",
    "                train_loader,\n",
    "                optimizer,\n",
    "                weighted_criterion,\n",
    "                device,\n",
    "                iteration=iteration,\n",
    "                saliency_generator=saliency_generator,\n",
    "                threshold=saliency_threshold\n",
    "            )\n",
    "            print(f\"  Época {epoch+1}/{epochs_per_iteration}, Perda: {loss:.4f}\")\n",
    "\n",
    "        # Avaliação\n",
    "        accuracy = evaluate_model(model, val_loader, device)\n",
    "        print(f\"  Acurácia de Validação (Pixel-wise): {accuracy:.4f}\")\n",
    "\n",
    "        # 5. Está bom? (Critério de parada: melhoria ou número máximo de iterações)\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            print(\"  Melhoria na acurácia. Continuar.\")\n",
    "        else:\n",
    "            # Critério de parada simplificado: se a acurácia não melhorar\n",
    "            print(\"  Acurácia não melhorou. Fim do treinamento iterativo.\")\n",
    "            # Se você quisesse um critério mais robusto, poderia verificar se\n",
    "            # a acurácia se estabilizou ou se o número máximo de iterações foi atingido.\n",
    "            # Aqui, vamos continuar até o final das iterações para demonstração.\n",
    "            # break # Descomente para parada antecipada\n",
    "\n",
    "    print(\"\\nPipeline de treinamento concluído.\")\n",
    "    print(f\"Melhor Acurácia de Validação: {best_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb8453e",
   "metadata": {},
   "source": [
    "### Execução Principal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6244126",
   "metadata": {},
   "source": [
    "- N_CLASSES -> Número de classes de segmentação\n",
    "- NUM_ITERATIONS -> Número de iterações do pipeline (Passos 2-5)\n",
    "- EPOCHS_PER_ITERATION -> Número de épocas por iteração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1e1e228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando pipeline de treinamento iterativo em cuda...\n",
      "\n",
      "--- Iteração 0 ---\n",
      "Fase 1: Treinamento inicial com segmentação semântica (tradicional).\n",
      "  Época 1/2, Perda: 1.6167\n",
      "  Época 2/2, Perda: 1.6097\n",
      "  Acurácia de Validação (Pixel-wise): 0.1998\n",
      "  Melhoria na acurácia. Continuar.\n",
      "\n",
      "--- Iteração 1 ---\n",
      "Fase 2-5: Re-treinamento com pesos de saliência.\n",
      "  Época 1/2, Perda: 0.0000\n",
      "  Época 2/2, Perda: 0.0000\n",
      "  Acurácia de Validação (Pixel-wise): 0.1999\n",
      "  Melhoria na acurácia. Continuar.\n",
      "\n",
      "--- Iteração 2 ---\n",
      "Fase 2-5: Re-treinamento com pesos de saliência.\n",
      "  Época 1/2, Perda: 0.0000\n",
      "  Época 2/2, Perda: 0.0000\n",
      "  Acurácia de Validação (Pixel-wise): 0.1995\n",
      "  Acurácia não melhorou. Fim do treinamento iterativo.\n",
      "\n",
      "Pipeline de treinamento concluído.\n",
      "Melhor Acurácia de Validação: 0.1999\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "N_CLASSES = 5\n",
    "IMG_SIZE = 128\n",
    "BATCH_SIZE = 4\n",
    "NUM_SAMPLES = 100\n",
    "NUM_ITERATIONS = 3\n",
    "EPOCHS_PER_ITERATION = 2\n",
    "\n",
    "model = UNet(n_channels=3, n_classes=N_CLASSES).to(DEVICE)\n",
    "\n",
    "train_dataset = DummySegmentationDataset(num_samples=NUM_SAMPLES, img_size=IMG_SIZE, n_classes=N_CLASSES)\n",
    "val_dataset = DummySegmentationDataset(num_samples=20, img_size=IMG_SIZE, n_classes=N_CLASSES)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "iterative_training_pipeline(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    DEVICE,\n",
    "    num_iterations=NUM_ITERATIONS,\n",
    "    epochs_per_iteration=EPOCHS_PER_ITERATION\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5159c36",
   "metadata": {},
   "source": [
    "O código gerado simula o pipeline proposto:\n",
    "1. Treinamento inicial (Iteração 0) com perda CrossEntropy padrão.\n",
    "2. Nas iterações seguintes, o SaliencyMapGenerator simula a criação do mapa de saliência.\n",
    "3. A função identify_relevant_areas simula a identificação das áreas mais relevantes.\n",
    "4. A função apply_spatial_reduction simula a criação de uma máscara de pesos.\n",
    "5. A WeightedCrossEntropyLoss usa essa máscara para ponderar a perda, dando mais foco às áreas 'saliêntes' no re-treinamento.\n",
    "\n",
    "Este é um esqueleto funcional que precisa ser adaptado com dados reais e uma implementação de saliência mais sofisticada (e.g., Grad-CAM) para um caso de uso prático."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
